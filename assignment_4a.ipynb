{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njits030/Odor_from_SMILES/blob/master/assignment_4a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ_pmgxvGur9"
      },
      "source": [
        "# Assignment 4a - Variational Auto-Encoders\n",
        "## Deep Learning Course - Vrije Universiteit Amsterdam, 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Stt4P3pE7og"
      },
      "source": [
        "### Notes by Marvin:\n",
        "\n",
        "- We were asked to implement encoder and decoder as FCNN or CNN, I did CNN because we get more points. Mind that we use a less common layer called \"deconv\", which is like the opposite of the convolution. We need it for increasing dimensions in the decoding process. Also note that the input dimensions in the layers are not fitting to our data, because I haven't gotten that far.\n",
        "\n",
        " - In Question 3, they ask us to choose a conditional lieklihood distribution. I think this is the correct one since our data is continuous. But if you can confirm that this is true.\n",
        "\n",
        "- In Question 5, they ask us to choose a prior distribution. Since these are a bit tricky, I am not sure which one to take here. Feel like I shouldn't decide by myself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEneMITS2agU"
      },
      "source": [
        "#### Instructions on how to use this notebook:\n",
        "\n",
        "This notebook is hosted on Google Colab. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
        "\n",
        "You can also avoid using Colab entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
        "\n",
        "The advantage of using Colab is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is very valuable because for various models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
        "\n",
        "The default Colab runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
        "\n",
        "```sh\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "Note that despite the name, Google Colab does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
        "\n",
        "**Submission:** Upload your notebook in .ipynb format to Canvas. The code and answers to the questions in the notebook are sufficient, no separate report is expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBgoJIpdLI2Y",
        "outputId": "d150e69b-dcfb-4ed6-cad1-f936591abc72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 23 14:42:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsdc7fDp40rQ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this assignment, we are going to implement a Variational Auto-Encoder (VAE). A VAE is a likelihood-based deep generative model that consists of a stochastic encoder (a variational posterior over latent variables), a stochastic decoder, and a marginal distribution over latent variables (a.k.a. a prior). The model was originally proposed in two concurrent papers:\n",
        "- [Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.](https://arxiv.org/abs/1312.6114)\n",
        "- [Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. \"Stochastic backpropagation and approximate inference in deep generative models.\" International conference on machine learning. PMLR, 2014.](https://proceedings.mlr.press/v32/rezende14.html)\n",
        "\n",
        "You can read more about VAEs in Chapter 4 of the following book:\n",
        "- [Tomczak, J.M., \"Deep Generative Modeling\", Springer, 2022](https://link.springer.com/book/10.1007/978-3-030-93158-2)\n",
        "\n",
        "In particular, the goals of this assignment are the following:\n",
        "\n",
        "- Understand how VAEs are formulated\n",
        "- Implement components of VAEs using PyTorch\n",
        "- Train and evaluate a model for image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvsuVNczG6pP"
      },
      "source": [
        "### Theory behind VAEs\n",
        "\n",
        "VAEs are latent variable models trained with variational inference. In general, the latent variable models define the following generative process:\n",
        "\\begin{align}\n",
        "1.\\ & \\mathbf{z} \\sim p_{\\lambda}(\\mathbf{z}) \\\\\n",
        "2.\\ & \\mathbf{x} \\sim p_{\\theta}(\\mathbf{x}|\\mathbf{z})\n",
        "\\end{align}\n",
        "\n",
        "In plain words, we assume that for observable data $\\mathbf{x}$, there are some latent (hidden) factors $\\mathbf{z}$. Then, the training objective is log-likelihood function of the following form:\n",
        "$$\n",
        "\\log p_{\\vartheta}(\\mathbf{x})=\\log \\int p_\\theta(\\mathbf{x} \\mid \\mathbf{z}) p_\\lambda(\\mathbf{z}) \\mathrm{d} \\mathbf{z} .\n",
        "$$\n",
        "\n",
        "The problem here is the intractability of the integral if the dependencies between random variables $\\mathbf{x}$ and $\\mathbf{z}$ are non-linear and/or the distributions are non-Gaussian.\n",
        "\n",
        "By introducing variational posteriors $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$, we get the following lower bound (the Evidence Lower Bound, ELBO):\n",
        "$$\n",
        "\\log p_{\\vartheta}(\\mathbf{x}) \\geq \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_\\theta(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\mathrm{KL}\\left(q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\| p_\\lambda(\\mathbf{z})\\right) .\n",
        "$$\n",
        "\n",
        "Note that we want to *maximize* this objective, therefore, in the code you are going to have to implement NELBO (negative ELBO) as a loss function (i.e., a minimization task)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suzhlbWqxtD9"
      },
      "source": [
        "## IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjxkigYLxpB7"
      },
      "outputs": [],
      "source": [
        "# DO NOT REMOVE!\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm23hRm6CqGh",
        "outputId": "3a5dcce5-28e5-4c8f-b0bf-6a719379e7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The available device is cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and determine the device\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(f'The available device is {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81CxONpmMulC",
        "outputId": "df979790-cec4-41cf-b1db-8a9c5f4ee439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# mount drive: WE NEED IT FOR SAVING IMAGES!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoPb92zNM4UY"
      },
      "outputs": [],
      "source": [
        "# PLEASE CHANGE IT TO YOUR OWN GOOGLE DRIVE!\n",
        "images_dir = '/content/gdrive/My Drive/DLresults/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3zs31tOyCmq"
      },
      "source": [
        "## Auxiliary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF0agzL7tDHK"
      },
      "source": [
        "Let us define some useful log-distributions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIBNVRNJtHSd"
      },
      "outputs": [],
      "source": [
        "# DO NOT REMOVE\n",
        "PI = torch.from_numpy(np.asarray(np.pi))\n",
        "EPS = 1.e-5\n",
        "\n",
        "\n",
        "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
        "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
        "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
        "    if reduction == 'avg':\n",
        "        return torch.mean(log_p, dim)\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(log_p, dim)\n",
        "    else:\n",
        "        return log_p\n",
        "\n",
        "\n",
        "def log_bernoulli(x, p, reduction=None, dim=None):\n",
        "    pp = torch.clamp(p, EPS, 1. - EPS)\n",
        "    log_p = x * torch.log(pp) + (1. - x) * torch.log(1. - pp)\n",
        "    if reduction == 'avg':\n",
        "        return torch.mean(log_p, dim)\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(log_p, dim)\n",
        "    else:\n",
        "        return log_p\n",
        "\n",
        "\n",
        "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
        "    D = x.shape[1]\n",
        "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
        "    if reduction == 'avg':\n",
        "        return torch.mean(log_p, dim)\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(log_p, dim)\n",
        "    else:\n",
        "        return log_p\n",
        "\n",
        "\n",
        "def log_standard_normal(x, reduction=None, dim=None):\n",
        "    D = x.shape[1]\n",
        "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * x**2.\n",
        "    if reduction == 'avg':\n",
        "        return torch.mean(log_p, dim)\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(log_p, dim)\n",
        "    else:\n",
        "        return log_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2LLOs0kn7iw"
      },
      "source": [
        "## Implementing VAEs\n",
        "\n",
        "The goal of this assignment is to implement four classes:\n",
        "- `Encoder`: this class implements the encoder (variational posterior), $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$.\n",
        "- `Decoder`: this class implements the decoded (the conditional likelihood), $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$.\n",
        "- `Prior`: this class implements the marginal over latents (the prior), $p_{\\lambda}(\\mathbf{z})$.\n",
        "- `VAE`: this class combines all components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnziXXLPJhp6"
      },
      "source": [
        "#### Question 0: (3 pt)\n",
        "**Fully-connected Neural Networks (MLPs) or Convolutional Neural Networks**\n",
        "\n",
        "This is not a real question but rather a comment. You are asked to implement your VAE using fully connected neural networks (MLPs) or convolutional neural networks (ConvNets).\n",
        "\n",
        "There is a difference in grading of this assignment based on your decision:\n",
        "- **If you decide to implement your VAE with MLPs and the model works properly, you get 1 pt.**\n",
        "- **If you decide to implement your VAE with ConvNets and the model works properly, you get 3 pts.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cXhOwKAzW6Z"
      },
      "source": [
        "### Encoder\n",
        "We start with `Encoder`. Please remember that we assume the Gaussian variational posterior with a diagonal covariance matrix.\n",
        "\n",
        "Feel free to add other methods to the class as well as arguments to the class initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrwQXSuEoFfH"
      },
      "outputs": [],
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, z_dim):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, hidden_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels * 2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Fully connected layers\n",
        "        self.flatten_size = hidden_channels * 2 * 7 * 7\n",
        "        self.fc_mu = nn.Linear(self.flatten_size, z_dim)\n",
        "        self.fc_log_var = nn.Linear(self.flatten_size, z_dim)\n",
        "\n",
        "    def reparameterization(self, mu, log_var):\n",
        "        # Sample z using the reparameterization trick\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        return mu + epsilon * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the encoder.\n",
        "        x: torch.tensor, with shape (batch_size, input_channels, height, width)\n",
        "        Returns: mu, log_var\n",
        "        \"\"\"\n",
        "        x = self.encoder(x).view(x.size(0), -1)  # Flatten\n",
        "        mu = self.fc_mu(x)\n",
        "        log_var = self.fc_log_var(x)\n",
        "        return mu, log_var\n",
        "\n",
        "    def sample(self, x=None, mu=None, log_var=None):\n",
        "        \"\"\"\n",
        "        Sample from the encoder. If `x` is provided, compute `mu` and `log_var`.\n",
        "        \"\"\"\n",
        "        if x is not None:\n",
        "            mu, log_var = self.forward(x)\n",
        "        return self.reparameterization(mu, log_var)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XVlH5OUzdgJ"
      },
      "source": [
        "Please answer the following questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1BNAH02zjjt"
      },
      "source": [
        "*italicized text*\n",
        "#### Question 1 (0.5 pt)\n",
        "\n",
        "Please explain the reparameterization trick and provide a mathematical formula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlxYq7-gzo_o"
      },
      "source": [
        "ANSWER: The encoder of a variational autoencoder maps input X to a latent distribution. Its outputs are the parameters to a gaussian distribution: The mean ($μ$) and variance ($\\sigma$). During training a latent vector $z$ is sampled from this distribution. $$ z \\sim N(μ, σ^2) $$\n",
        "\n",
        "However, the sampling introduces randomness and the randomness does not allow us to compute the derivatives to update the network weights. So instead of sampling from this unknown distribution $N(μ, σ^2)$ we sample from a distriution that we do know: $ ϵ \\sim N(0, 1)$. We can than reparameterize the sample that we drew from the standard normal distribution using the μ and σ from the unknown distribution with the following formula: $z = μ + ϵ * σ$. By doing this we have isolated the randomness in epsilon which is independent of the model and μ and σ are now deterministic variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpRgXdtBzt3-"
      },
      "source": [
        "After sampling a latent representation $z$ from $q(z|x)$ with specific mean vector $\\mu$ and covariance matrix $\\Sigma$, we can test how likely said $z$ is under $q(z|x)$ by passing it through the probability density function of the multivariate Gaussian $q(z|x)$. The probability density function is:\n",
        "\n",
        "$$\n",
        "p(z|x) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp\\left(-\\frac{1}{2} (z - \\mu)^T \\Sigma^{-1} (z - \\mu)\\right),\n",
        "$$\n",
        "\n",
        "where $d$ is the dimensionality of $z$, $|\\Sigma|$ is the determinant of the covariance matrix $\\Sigma$, and $\\Sigma^{-1}$ is the inverse of the covariance matrix.\n",
        "\n",
        "Taking the logarithm, we arrive at:\n",
        "\n",
        "$$\n",
        "\\log p(z|x) = -\\frac{d}{2} \\log(2\\pi) - \\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} (z - \\mu)^T \\Sigma^{-1} (z - \\mu).\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhNTy5mn0XDT"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder is the conditional likelihood, i.e., $p(\\mathbf{x}|\\mathbf{z})$. Please remember that we must decide on the form of the distribution (e.g., Bernoulli, Gaussian, Categorical). Please discuss it with a TA or a lecturer if you are in doubt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vTmKHwrpUVa"
      },
      "outputs": [],
      "source": [
        "class CNNDecoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_channels, output_channels, output_size):\n",
        "        super(CNNDecoder, self).__init__()\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.fc = nn.Linear(z_dim, hidden_channels * 7 * 7)\n",
        "\n",
        "        # Deconvolutional layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_channels, hidden_channels // 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels // 2, output_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()  # Output in range [0, 1] for Bernoulli\n",
        "        )\n",
        "\n",
        "    def sample(self, z):\n",
        "        \"\"\"\n",
        "        For a given latent code, compute Bernoulli probabilities and sample x ~ p(x|z).\n",
        "        \"\"\"\n",
        "        x = self.fc(z).view(-1, self.hidden_channels, 7, 7)\n",
        "        x = self.decoder(x)\n",
        "        return torch.bernoulli(x)  # Sample binary data from Bernoulli distribution\n",
        "\n",
        "    def forward(self, z, x):\n",
        "        \"\"\"\n",
        "        Compute the log probability: log p(x|z).\n",
        "        \"\"\"\n",
        "        x_reconstructed = self.fc(z).view(-1, self.hidden_channels, 7, 7)\n",
        "        x_reconstructed = self.decoder(x_reconstructed)\n",
        "\n",
        "        # Compute log-probability using binary cross-entropy\n",
        "        log_prob = -F.binary_cross_entropy(x_reconstructed, x, reduction=\"none\")\n",
        "        log_prob = log_prob.sum(dim=[1, 2, 3])  # Sum over all dimensions except batch\n",
        "\n",
        "        return log_prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xjbNkNL01DP"
      },
      "source": [
        "Please answer the following questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjDvPaBj04cA"
      },
      "source": [
        "#### Question 3 (0.5 pt)\n",
        "\n",
        "Please explain your choice of the distribution for image data used in this assignment. Additionally, please write it down mathematically (if you think that presenting it as the log-probability, then please do it)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLZEzmGI1Ok-"
      },
      "source": [
        "ANSWER:\n",
        "\n",
        "Since the MNIST dataset is grayscale, our data is continuous due to the fact that values can range from 0 to 1. Because of this, we reasoned that a Gaussian distribution might be a fitting distribution, as compared to other discrete distributions such as Bernoulli. The equation for the log probability given our chosen conditional likelihood estimator is based on the PDF of the multivariate Gaussian distribution. However, we interchange $x$ and $z$ as compared to our variational posterior.\n",
        "\n",
        "$$\n",
        "p(x|z) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma(z)|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu(z))^T \\Sigma(z)^{-1} (x - \\mu(z))\\right).\n",
        "$$\n",
        "\n",
        "In order to get the log-probability, we take the log on both sides, arriving at:\n",
        "\n",
        "$$\n",
        "\\log p(x|z) = -\\frac{d}{2} \\log(2\\pi) - \\frac{1}{2} \\log |\\Sigma(z)| - \\frac{1}{2} (x - \\mu(z))^T \\Sigma(z)^{-1} (x - \\mu(z)).\n",
        "$$\n",
        "\n",
        "Note that, in our code, the formula for the log-probability looks slightly different. The difference arises because the code assumes a diagonal covariance matrix, simplifying the multivariate Gaussian's quadratic term and determinant. Specifically, the quadratic term $(x - \\mu)^T \\Sigma^{-1} (x - \\mu)$ reduces to an element-wise operation $\\frac{(x - \\mu)^2}{\\sigma^2}$, and $\\log |\\Sigma|$ simplifies to $\\sum \\log(\\sigma^2)$, making computations more efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhbWamId1eGt"
      },
      "source": [
        "#### Question 4 (0.5 pt)\n",
        "\n",
        "Please explain how one can sample from the distribution chosen by you. Please be specific and formal (i.e., provide mathematical formulae). If applicable, please provide a code snippet.\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "Just like when sampling $z$ from the variational posterior, we again utilize the reparametrization trick, meaning we first sample an instance $x$ from our conditional likelihood distribution, the parameters of which are given by a forward pass through the decoder model. We then express $x$ as conditioned on $z$ by making it dependent on the mean and variance of the conditional likelihood:\n",
        "\n",
        "$$\n",
        "x = \\mu (z) + \\sigma (z) \\cdot ϵ'\n",
        "$$\n",
        "\n",
        "where $$\\epsilon' \\sim N(0, 1).$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLIEwIiw00op"
      },
      "source": [
        "### Prior\n",
        "\n",
        "The prior is the marginal distribution over latent variables, i.e., $p(\\mathbf{z})$. It plays a crucial role in the generative process and also in synthesizing images of a better quality.\n",
        "\n",
        "In this assignment, you are asked to implement a prior that is learnable (e.g., parameterized by neural networks). If you decide to implement the standard Gaussian prior only, then please be aware that you will not get any points.\n",
        "\n",
        "\n",
        "For the learnable prior you can choose one of the following options:\n",
        "\n",
        "\n",
        "*   Mixture of Gaussians\n",
        "*   Normalizing Flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQIvee5Cp69V"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE GOES HERE\n",
        "# NOTES:\n",
        "# (i) Implementing the standard Gaussian prior does not give you any points!\n",
        "# (ii) The function \"sample\" must be implemented.\n",
        "# (iii) The function \"forward\" must return the log-probability, i.e., log p(z)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from torch.distributions.transforms import Transform, ComposeTransform\n",
        "\n",
        "class PlanarFlow(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.u = nn.Parameter(torch.randn(1, dim))\n",
        "        self.w = nn.Parameter(torch.randn(1, dim))\n",
        "        self.b = nn.Parameter(torch.randn(1))\n",
        "        self.scale = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"Forward transform with improved stability.\"\"\"\n",
        "        linear = F.linear(z, self.w, self.b)\n",
        "        return z + self.scale * (self.u * torch.tanh(linear))\n",
        "\n",
        "    def log_abs_det_jacobian(self, z):\n",
        "        \"\"\"Directly compute log det Jacobian without inverse.\"\"\"\n",
        "        linear = F.linear(z, self.w, self.b)\n",
        "        psi = (1.0 - torch.tanh(linear).pow(2)) * self.w\n",
        "        det = 1.0 + self.scale * torch.sum(self.u * psi, dim=-1)\n",
        "        return torch.log(torch.abs(det) + 1e-6)\n",
        "\n",
        "class FlowPrior(nn.Module):\n",
        "    def __init__(self, z_dim=2, num_flows=5, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.device = device\n",
        "        self.flows = nn.ModuleList([PlanarFlow(z_dim) for _ in range(num_flows)])\n",
        "        self.base_dist = Normal(torch.zeros(z_dim).to(device), torch.ones(z_dim).to(device))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Draw z0 from a standard Normal, then transform it: zK = f(...f(z0)).\"\"\"\n",
        "        z = self.base_dist.sample((batch_size,))\n",
        "        for flow in self.flows:\n",
        "            z = flow(z)\n",
        "        return z\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"More efficient forward pass using direct computation.\"\"\"\n",
        "        log_det_sum = 0\n",
        "        for flow in reversed(self.flows):\n",
        "            log_det_sum += flow.log_abs_det_jacobian(z)\n",
        "            z = flow(z)\n",
        "        return self.base_dist.log_prob(z).sum(-1) + log_det_sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0KH9f2O_PDg"
      },
      "source": [
        "#### Question 5 (2 pts max)\n",
        "\n",
        "**Option 1 (0 pt):  Standard Gaussian**\n",
        "\n",
        "**NOTE: *If you decide to use the standard Gaussian prior, please indicate it in your answer. However, you will get 0 pt for this question.***\n",
        "\n",
        "**Option 2 (0.5 pt): Mixture of Gaussains**\n",
        "\n",
        "Please do the following:\n",
        "- (0.25 pt) Please explain your prior and write it down mathematically\n",
        "- (0.15 pt) Please write down its sampling procedure (if necessary, please add a code snippet).\n",
        "- (0.1 pt) Please write down its log-probability (a mathematical formula).\n",
        "\n",
        "**Option 3 (2 pts): Normalizing Flow**\n",
        "\n",
        "Please do the following:\n",
        "- (1 pt) Please explain your prior and write it down mathematically\n",
        "- (0.5 pt) Please write down its sampling procedure (if necessary, please add a code snippet).\n",
        "- (0.5 pt) Please write down its log-probability (a mathematical formula)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "132db304alVN"
      },
      "source": [
        "Our prior is a normalizing flow that sarts from a base distribution of $z_0 \\sim N(0, I)$ in $\\mathbb{R}^d$ and sequentially applies a chain of invertible transformations $f_1, f_2, \\ldots, f_K$, so that the final variable $z=f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1\\left(z_0\\right)$ can model a more flexible distribution.\n",
        "\n",
        "To sample from this prior distribution, we first draw $z_0$ from the base normal distribution, then apply each transformation in order $z=f_K\\left(f_{K-1}\\left(\\ldots f_1\\left(z_0\\right) \\ldots\\right)\\right)$, to eventually return $z$. The PyTorch code snippet would look like,\n",
        "\n",
        "```python\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Draw z0 from a standard Normal, then transform it: zK = f(...f(z0)).\n",
        "        \"\"\"\n",
        "        z0 = self.base_dist.sample((batch_size,))  # shape [batch_size, z_dim]\n",
        "        z = z0\n",
        "        for transform in self.flow_transforms:\n",
        "            z = transform(z)  # apply forward transform\n",
        "        return z\n",
        "```\n",
        "\n",
        "The log-density of any point $z$ under the flow is,\n",
        "\n",
        "$$\n",
        "\\log p(z)=\\log p\\left(z_0\\right) \\sum_{k=1}^K \\log \\left|\\operatorname{det} \\frac{\\partial f_k}{\\partial z_{k-1}}\\right|, \\\\\n",
        "$$\n",
        "\n",
        "where $z_0=f_1^{-1}(f_2^{-1}(\\ldots f_K^{-1}(z) \\ldots))$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOM4QM9I_62d"
      },
      "source": [
        "### Complete VAE\n",
        "\n",
        "The last class is `VAE` tha combines all components. Please remember that this class must implement the **Negative ELBO** in `forward`, as well as `sample` (*hint*: it is a composition of `sample` functions from the prior and the decoder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQpf-BeSqA6V"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE GOES HERE\n",
        "# This class combines Encoder, Decoder and Prior.\n",
        "# NOTES:\n",
        "# (i) The function \"sample\" must be implemented.\n",
        "# (ii) The function \"forward\" must return the negative ELBO. Please remember to add an argument \"reduction\" that is either \"mean\" or \"sum\".\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import pi\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, encoder, decoder, prior, device='cuda'):\n",
        "        \"\"\"\n",
        "        encoder: CNNEncoder instance\n",
        "            Must implement forward(x) -> (mu, logvar)\n",
        "        decoder: CNNDecoder instance\n",
        "            Must implement forward(z, x) -> log p(x|z) and sample(z) -> x_sample\n",
        "        prior: FlowPrior (or another prior) instance\n",
        "            Must implement sample(batch_size) -> z and forward(z) -> log p(z)\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.prior = prior\n",
        "        self.device = device\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "        \"\"\"\n",
        "        1) Sample z ~ prior\n",
        "        2) Decode x ~ p(x|z)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Sample latent codes from the flow prior\n",
        "            z = self.prior.sample(batch_size)  # shape [batch_size, z_dim]\n",
        "            # Use the decoder's 'sample' method to sample x\n",
        "            x_sample = self.decoder.sample(z)  # shape [batch_size, output_channels, H, W]\n",
        "        return x_sample\n",
        "\n",
        "    def forward(self, x, reduction='mean'):\n",
        "        \"\"\"\n",
        "        Compute the Negative ELBO = -E_{q(z|x)}[log p(x|z)] + KL(q(z|x) || p(z)).\n",
        "\n",
        "        Input:\n",
        "            x: input images, shape [batch_size, input_channels, H, W]\n",
        "            reduction: 'mean' or 'sum'\n",
        "        Returns:\n",
        "            nelbo: negative ELBO, either averaged or summed over batch.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Encode x -> (mu, logvar)\n",
        "        mu, logvar = self.encoder(x)  # each shape [batch_size, z_dim]\n",
        "\n",
        "        # Reparameterize to get z ~ q(z|x)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + std * eps  # shape [batch_size, z_dim]\n",
        "\n",
        "        # Reconstruction term: - E_{q(z|x)} [ log p(x|z) ]\n",
        "        #    We can use the decoder's forward pass, which returns log p(x|z).\n",
        "        log_p_x_given_z = self.decoder(z, x)  # shape [batch_size]\n",
        "        recon_loss = -log_p_x_given_z  # negative log-likelihood\n",
        "\n",
        "        # KL term: KL[q(z|x) || p(z)]\n",
        "        #    = E_{q(z|x)} [ log q(z|x) - log p(z) ]\n",
        "        #\n",
        "        #    log q(z|x) for a diagonal Gaussian:\n",
        "        #    = sum_{i=1}^d -0.5 [ log(2π) + log σ_i^2 + (z_i - μ_i)^2 / σ_i^2 ]\n",
        "        #\n",
        "        #    log p(z) is from the FlowPrior, which computes log p(z) = log p(z0) + ...\n",
        "        # Compute log q(z|x)\n",
        "        z_dim = mu.shape[1]\n",
        "        log_q_z_given_x = -0.5 * (\n",
        "            z_dim * torch.log(torch.tensor(2.0 * pi)) +\n",
        "            torch.sum(logvar, dim=1) +\n",
        "            torch.sum((z - mu)**2 / torch.exp(logvar), dim=1)\n",
        "        )\n",
        "\n",
        "        # Compute log p(z)\n",
        "        log_p_z = self.prior(z)  # shape [batch_size]\n",
        "\n",
        "        # KL = E_{q(z|x)} [ log q(z|x) - log p(z) ]\n",
        "        kl = log_q_z_given_x - log_p_z\n",
        "\n",
        "        # Negative ELBO per sample\n",
        "        #    NELBO = recon_loss + kl\n",
        "        nelbo = recon_loss + kl  # shape [batch_size]\n",
        "\n",
        "        # Reduction\n",
        "        if reduction == 'sum':\n",
        "            return nelbo.sum()\n",
        "        else:\n",
        "            return nelbo.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaJgXPYyAmeJ"
      },
      "source": [
        "#### Question 6 (0.5 pt)\n",
        "\n",
        "Please write down mathematically the **Negative ELBO** and provide a code snippet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THYyO-G7AkSQ"
      },
      "source": [
        "$$\n",
        "\\text{NELBO}\n",
        "= - \\mathbb{E}_{q(z \\mid x)} \\bigl[ \\log p(x \\mid z) \\bigr]\n",
        "\t⋅\t\\mathrm{KL}\\bigl(q(z \\mid x) \\,\\|\\, p(z)\\bigr)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLhgze7DA4yx"
      },
      "source": [
        "### Evaluation and training functions\n",
        "\n",
        "**Please do not remove or modify them.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Dr3a6lqJ0W"
      },
      "outputs": [],
      "source": [
        "# DO NOT REMOVE\n",
        "\n",
        "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
        "    # EVALUATION\n",
        "    if model_best is None:\n",
        "        # load best performing model\n",
        "        model_best = torch.load(name + '.model')\n",
        "\n",
        "    model_best.eval()\n",
        "    loss = 0.\n",
        "    N = 0.\n",
        "    for indx_batch, (test_batch, _) in enumerate(test_loader):\n",
        "        test_batch = test_batch.to(device)\n",
        "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
        "        loss = loss + loss_t.item()\n",
        "        N = N + test_batch.shape[0]\n",
        "    loss = loss / N\n",
        "\n",
        "    if epoch is None:\n",
        "        print(f'FINAL LOSS: nll={loss}')\n",
        "    else:\n",
        "        print(f'Epoch: {epoch}, val nll={loss}')\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def samples_real(name, test_loader, shape=(28,28)):\n",
        "    # real images-------\n",
        "    num_x = 4\n",
        "    num_y = 4\n",
        "    x, _ = next(iter(test_loader))\n",
        "    x = x.to('cpu').detach().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(num_x, num_y)\n",
        "    for i, ax in enumerate(ax.flatten()):\n",
        "        plottable_image = np.reshape(x[i], shape)\n",
        "        ax.imshow(plottable_image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def samples_generated(name, data_loader, shape=(28,28), extra_name=''):\n",
        "    x, _ = next(iter(data_loader))\n",
        "    x = x.to('cpu').detach().numpy()\n",
        "\n",
        "    # generations-------\n",
        "    model_best = torch.load(name + '.model')\n",
        "    model_best.eval()\n",
        "\n",
        "    num_x = 4\n",
        "    num_y = 4\n",
        "    x = model_best.sample(num_x * num_y)\n",
        "    x = x.to('cpu').detach().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(num_x, num_y)\n",
        "    for i, ax in enumerate(ax.flatten()):\n",
        "        plottable_image = np.reshape(x[i], shape)\n",
        "        ax.imshow(plottable_image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_curve(name, nll_val):\n",
        "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('nll')\n",
        "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ABgMeG0qFAP"
      },
      "outputs": [],
      "source": [
        "# DO NOT REMOVE\n",
        "\n",
        "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader, shape=(28,28)):\n",
        "    nll_val = []\n",
        "    best_nll = 1000.\n",
        "    patience = 0\n",
        "\n",
        "    # Main loop\n",
        "    for e in range(num_epochs):\n",
        "        print(f\"e - {e}\")\n",
        "        # TRAINING\n",
        "        model.train()\n",
        "        for indx_batch, (batch, _) in enumerate(training_loader):\n",
        "            batch = batch.to(device)\n",
        "            loss = model.forward(batch, reduction='mean')\n",
        "            if indx_batch % 100 == 0:\n",
        "              print(f\"e - {e}, batch - {indx_batch}, loss - {loss.item()}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
        "        print(f\"e - {e}, val loss - {loss_val}\")\n",
        "        nll_val.append(loss_val)  # save for plotting\n",
        "\n",
        "        if e == 0:\n",
        "            print('saved!')\n",
        "            torch.save(model, name + '.model')\n",
        "            best_nll = loss_val\n",
        "        else:\n",
        "            if loss_val < best_nll:\n",
        "                print('saved!')\n",
        "                torch.save(model, name + '.model')\n",
        "                best_nll = loss_val\n",
        "                patience = 0\n",
        "\n",
        "                samples_generated(name, val_loader, shape=shape, extra_name=\"_epoch_\" + str(e))\n",
        "            else:\n",
        "                patience = patience + 1\n",
        "\n",
        "        if patience > max_patience:\n",
        "            break\n",
        "\n",
        "    nll_val = np.asarray(nll_val)\n",
        "\n",
        "    return nll_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWr8N2u2qNTu"
      },
      "source": [
        "### Setup\n",
        "\n",
        "**NOTE: *Please comment your code! Especially if you introduce any new variables (e.g., hyperparameters).***\n",
        "\n",
        "In the following cells, we define `transforms` for the dataset. Next, we initialize the data, a directory for results and some fixed hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFTE5jtYpxDV"
      },
      "outputs": [],
      "source": [
        "# PLEASE DEFINE APPROPRIATE TRANFORMS FOR THE DATASET\n",
        "from torchvision import transforms\n",
        "\n",
        "# NEED TO CHANGE TO PROPER TRANSFORMS\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SDcOBbGCM8z"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Please do not modify the code in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXHitzrYqNhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5dbcbb-62f2-4198-a652-e354bdc3bff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/MNIST/raw/train-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /files/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 506kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/MNIST/raw/train-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.58MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/MNIST/raw/t10k-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.16MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/MNIST/raw/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DO NOT REMOVE\n",
        "#-dataset\n",
        "dataset = MNIST('/files/', train=True, download=True,\n",
        "                      transform=transforms_train\n",
        "                )\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [50000, 10000], generator=torch.Generator().manual_seed(14))\n",
        "\n",
        "test_dataset = MNIST('/files/', train=False, download=True,\n",
        "                      transform=transforms_test\n",
        "                     )\n",
        "#-dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#-creating a dir for saving results\n",
        "name = 'vae'\n",
        "result_dir = os.path.join(images_dir, 'results', name)\n",
        "if not(os.path.exists(result_dir)):\n",
        "    os.mkdir(result_dir)\n",
        "\n",
        "#-hyperparams (please do not modify them for the final report)\n",
        "num_epochs = 1000 # max. number of epochs\n",
        "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmKDXMI0B231"
      },
      "source": [
        "In the next cell, please initialize the model. Please remember about commenting your code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b73aaBDxqSYb",
        "outputId": "8fd25942-a82c-404d-ce00-181fe9dfb857"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (encoder): CNNEncoder(\n",
              "    (encoder): Sequential(\n",
              "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): ReLU()\n",
              "      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (3): ReLU()\n",
              "    )\n",
              "    (fc_mu): Linear(in_features=3136, out_features=4, bias=True)\n",
              "    (fc_log_var): Linear(in_features=3136, out_features=4, bias=True)\n",
              "  )\n",
              "  (decoder): CNNDecoder(\n",
              "    (fc): Linear(in_features=4, out_features=1568, bias=True)\n",
              "    (decoder): Sequential(\n",
              "      (0): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): ReLU()\n",
              "      (2): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): Sigmoid()\n",
              "    )\n",
              "  )\n",
              "  (prior): FlowPrior(\n",
              "    (flows): ModuleList(\n",
              "      (0-4): 5 x PlanarFlow()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Instantiate the components\n",
        "encoder = CNNEncoder(input_channels=1, hidden_channels=32, z_dim=4).to(device) # Move encoder to device\n",
        "decoder = CNNDecoder(z_dim=4, hidden_channels=32, output_channels=1, output_size=28).to(device) # Move decoder to device\n",
        "flow_prior = FlowPrior(z_dim=4, num_flows=5, device=device).to(device) # Move prior to device\n",
        "\n",
        "# Combine into a VAE model\n",
        "model = VAE(encoder=encoder, decoder=decoder, prior=flow_prior, device=device).to(device) # Move VAE to device\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC8AkWt4CURT"
      },
      "source": [
        "Please initialize the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3nTSDe7ql08",
        "outputId": "c4416f6a-d4b3-4c42-ee8e-39c6b6df03c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# PLEASE DEFINE YOUR OPTIMIZER\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79odxtRjCaix"
      },
      "source": [
        "#### Question 7 (0.5 pt)\n",
        "\n",
        "Please explain the choice of the optimizer, and comment on the choice of the hyperparameters (e.g., the learing reate value)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEjOlYN9Ft_B"
      },
      "source": [
        "ANSWER: We chose Adam as our optimizer because it dynamically adapts learning rates for each parameter making the training more stable and efficient. VAEs optimize both the reconstruction loss and KL divergence which can have differing magnitudes and gradients. Adam balances these components by smoothing noisy gradients using its momentum terms and scaling updates adaptively. This is important due to the stochastic nature of VAEs introduced by the reparameterization trick. Furthermore, Adam performs well in non-stationary loss landscapes and requires minimal hyperparameter tuning, making it a good choice for VAEs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5GrzUcHFweG"
      },
      "source": [
        "### Training and final evaluation\n",
        "\n",
        "In the following two cells, we run the training and the final evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD7WuY6bqnBK",
        "outputId": "3ff0cebc-495a-4a48-8b4f-9a508f1ed02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e - 0\n",
            "e - 0, batch - 0, loss - 563.719482421875\n",
            "e - 0, batch - 100, loss - 217.77554321289062\n",
            "e - 0, batch - 200, loss - 193.1779022216797\n",
            "e - 0, batch - 300, loss - 182.01242065429688\n",
            "e - 0, batch - 400, loss - 155.80377197265625\n",
            "e - 0, batch - 500, loss - 177.58950805664062\n",
            "e - 0, batch - 600, loss - 166.4644317626953\n",
            "e - 0, batch - 700, loss - 161.02545166015625\n",
            "e - 0, batch - 800, loss - 172.87841796875\n",
            "e - 0, batch - 900, loss - 153.3182830810547\n",
            "e - 0, batch - 1000, loss - 159.89297485351562\n",
            "e - 0, batch - 1100, loss - 160.169189453125\n",
            "e - 0, batch - 1200, loss - 167.84274291992188\n",
            "e - 0, batch - 1300, loss - 137.34913635253906\n",
            "e - 0, batch - 1400, loss - 138.3892364501953\n",
            "e - 0, batch - 1500, loss - 144.32330322265625\n",
            "Epoch: 0, val nll=149.1629515625\n",
            "e - 0, val loss - 149.1629515625\n",
            "saved!\n",
            "e - 1\n",
            "e - 1, batch - 0, loss - 153.97715759277344\n",
            "e - 1, batch - 100, loss - 161.44317626953125\n",
            "e - 1, batch - 200, loss - 153.38687133789062\n",
            "e - 1, batch - 300, loss - 144.59136962890625\n",
            "e - 1, batch - 400, loss - 151.81997680664062\n",
            "e - 1, batch - 500, loss - 153.3232421875\n",
            "e - 1, batch - 600, loss - 154.53907775878906\n",
            "e - 1, batch - 700, loss - 133.37881469726562\n",
            "e - 1, batch - 800, loss - 137.304931640625\n",
            "e - 1, batch - 900, loss - 130.23651123046875\n",
            "e - 1, batch - 1000, loss - 143.8005828857422\n",
            "e - 1, batch - 1100, loss - 145.44647216796875\n",
            "e - 1, batch - 1200, loss - 141.3214569091797\n",
            "e - 1, batch - 1300, loss - 141.4136505126953\n",
            "e - 1, batch - 1400, loss - 147.54656982421875\n",
            "e - 1, batch - 1500, loss - 136.13375854492188\n",
            "Epoch: 1, val nll=142.67940616455078\n",
            "e - 1, val loss - 142.67940616455078\n",
            "saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-d6ac2cd6f38d>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_best = torch.load(name + '.model')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e - 2\n",
            "e - 2, batch - 0, loss - 147.32858276367188\n",
            "e - 2, batch - 100, loss - 143.14723205566406\n",
            "e - 2, batch - 200, loss - 134.77880859375\n",
            "e - 2, batch - 300, loss - 139.90391540527344\n",
            "e - 2, batch - 400, loss - 146.57115173339844\n",
            "e - 2, batch - 500, loss - 135.63015747070312\n",
            "e - 2, batch - 600, loss - 140.24977111816406\n",
            "e - 2, batch - 700, loss - 145.89456176757812\n",
            "e - 2, batch - 800, loss - 142.03213500976562\n",
            "e - 2, batch - 900, loss - 148.29544067382812\n",
            "e - 2, batch - 1000, loss - 137.85372924804688\n",
            "e - 2, batch - 1100, loss - 143.29632568359375\n",
            "e - 2, batch - 1200, loss - 124.9944839477539\n",
            "e - 2, batch - 1300, loss - 155.89694213867188\n",
            "e - 2, batch - 1400, loss - 154.27859497070312\n",
            "e - 2, batch - 1500, loss - 127.33628845214844\n",
            "Epoch: 2, val nll=140.27544786376953\n",
            "e - 2, val loss - 140.27544786376953\n",
            "saved!\n",
            "e - 3\n",
            "e - 3, batch - 0, loss - 145.2783966064453\n",
            "e - 3, batch - 100, loss - 130.11331176757812\n",
            "e - 3, batch - 200, loss - 125.09447479248047\n",
            "e - 3, batch - 300, loss - 130.85833740234375\n",
            "e - 3, batch - 400, loss - 155.01708984375\n",
            "e - 3, batch - 500, loss - 150.849365234375\n",
            "e - 3, batch - 600, loss - 146.00657653808594\n",
            "e - 3, batch - 700, loss - 139.51910400390625\n",
            "e - 3, batch - 800, loss - 140.215576171875\n",
            "e - 3, batch - 900, loss - 143.35816955566406\n",
            "e - 3, batch - 1000, loss - 137.35948181152344\n",
            "e - 3, batch - 1100, loss - 135.798095703125\n",
            "e - 3, batch - 1200, loss - 145.78414916992188\n",
            "e - 3, batch - 1300, loss - 138.05636596679688\n",
            "e - 3, batch - 1400, loss - 150.7213134765625\n",
            "e - 3, batch - 1500, loss - 148.30661010742188\n",
            "Epoch: 3, val nll=138.81557010498048\n",
            "e - 3, val loss - 138.81557010498048\n",
            "saved!\n",
            "e - 4\n",
            "e - 4, batch - 0, loss - 139.99240112304688\n",
            "e - 4, batch - 100, loss - 143.7561492919922\n",
            "e - 4, batch - 200, loss - 139.62664794921875\n",
            "e - 4, batch - 300, loss - 122.7164535522461\n",
            "e - 4, batch - 400, loss - 154.6405029296875\n",
            "e - 4, batch - 500, loss - 142.62905883789062\n",
            "e - 4, batch - 600, loss - 149.4849395751953\n",
            "e - 4, batch - 700, loss - 152.11990356445312\n",
            "e - 4, batch - 800, loss - 157.6853485107422\n",
            "e - 4, batch - 900, loss - 120.29838562011719\n",
            "e - 4, batch - 1000, loss - 134.69061279296875\n",
            "e - 4, batch - 1100, loss - 146.91586303710938\n",
            "e - 4, batch - 1200, loss - 145.86996459960938\n",
            "e - 4, batch - 1300, loss - 146.90553283691406\n",
            "e - 4, batch - 1400, loss - 136.66796875\n",
            "e - 4, batch - 1500, loss - 142.4575653076172\n",
            "Epoch: 4, val nll=138.3675634765625\n",
            "e - 4, val loss - 138.3675634765625\n",
            "saved!\n",
            "e - 5\n",
            "e - 5, batch - 0, loss - 143.54541015625\n",
            "e - 5, batch - 100, loss - 143.1265869140625\n",
            "e - 5, batch - 200, loss - 158.325927734375\n",
            "e - 5, batch - 300, loss - 139.05828857421875\n",
            "e - 5, batch - 400, loss - 140.74876403808594\n",
            "e - 5, batch - 500, loss - 144.59251403808594\n",
            "e - 5, batch - 600, loss - 140.10098266601562\n",
            "e - 5, batch - 700, loss - 139.08033752441406\n",
            "e - 5, batch - 800, loss - 123.81494903564453\n",
            "e - 5, batch - 900, loss - 137.15380859375\n",
            "e - 5, batch - 1000, loss - 141.92860412597656\n",
            "e - 5, batch - 1100, loss - 127.47685241699219\n",
            "e - 5, batch - 1200, loss - 142.73980712890625\n",
            "e - 5, batch - 1300, loss - 129.81756591796875\n",
            "e - 5, batch - 1400, loss - 137.69671630859375\n",
            "e - 5, batch - 1500, loss - 139.55223083496094\n",
            "Epoch: 5, val nll=137.95486110839843\n",
            "e - 5, val loss - 137.95486110839843\n",
            "saved!\n",
            "e - 6\n",
            "e - 6, batch - 0, loss - 126.81967163085938\n",
            "e - 6, batch - 100, loss - 147.28221130371094\n",
            "e - 6, batch - 200, loss - 139.95693969726562\n",
            "e - 6, batch - 300, loss - 137.35455322265625\n",
            "e - 6, batch - 400, loss - 148.62611389160156\n",
            "e - 6, batch - 500, loss - 132.99789428710938\n",
            "e - 6, batch - 600, loss - 150.5716552734375\n",
            "e - 6, batch - 700, loss - 145.8033447265625\n",
            "e - 6, batch - 800, loss - 140.04598999023438\n",
            "e - 6, batch - 900, loss - 144.75424194335938\n",
            "e - 6, batch - 1000, loss - 133.99658203125\n",
            "e - 6, batch - 1100, loss - 136.74278259277344\n",
            "e - 6, batch - 1200, loss - 139.5647735595703\n",
            "e - 6, batch - 1300, loss - 140.21188354492188\n",
            "e - 6, batch - 1400, loss - 142.10922241210938\n",
            "e - 6, batch - 1500, loss - 134.60279846191406\n",
            "Epoch: 6, val nll=136.86583134765624\n",
            "e - 6, val loss - 136.86583134765624\n",
            "saved!\n",
            "e - 7\n",
            "e - 7, batch - 0, loss - 141.46621704101562\n",
            "e - 7, batch - 100, loss - 144.14248657226562\n",
            "e - 7, batch - 200, loss - 139.2312469482422\n",
            "e - 7, batch - 300, loss - 145.88677978515625\n",
            "e - 7, batch - 400, loss - 133.3873748779297\n",
            "e - 7, batch - 500, loss - 126.28023529052734\n",
            "e - 7, batch - 600, loss - 132.03256225585938\n",
            "e - 7, batch - 700, loss - 128.55226135253906\n",
            "e - 7, batch - 800, loss - 148.2667694091797\n",
            "e - 7, batch - 900, loss - 128.83734130859375\n",
            "e - 7, batch - 1000, loss - 131.76248168945312\n",
            "e - 7, batch - 1100, loss - 160.1736602783203\n",
            "e - 7, batch - 1200, loss - 143.33090209960938\n",
            "e - 7, batch - 1300, loss - 133.1522216796875\n",
            "e - 7, batch - 1400, loss - 128.15496826171875\n",
            "e - 7, batch - 1500, loss - 138.2230224609375\n",
            "Epoch: 7, val nll=136.3318642578125\n",
            "e - 7, val loss - 136.3318642578125\n",
            "saved!\n",
            "e - 8\n",
            "e - 8, batch - 0, loss - 127.52969360351562\n",
            "e - 8, batch - 100, loss - 123.22421264648438\n",
            "e - 8, batch - 200, loss - 152.9291229248047\n",
            "e - 8, batch - 300, loss - 130.4993896484375\n",
            "e - 8, batch - 400, loss - 147.45065307617188\n",
            "e - 8, batch - 500, loss - 125.66217803955078\n",
            "e - 8, batch - 600, loss - 132.73947143554688\n",
            "e - 8, batch - 700, loss - 144.0534210205078\n",
            "e - 8, batch - 800, loss - 140.17572021484375\n",
            "e - 8, batch - 900, loss - 137.7029266357422\n",
            "e - 8, batch - 1000, loss - 133.46505737304688\n",
            "e - 8, batch - 1100, loss - 136.73471069335938\n",
            "e - 8, batch - 1200, loss - 144.56309509277344\n",
            "e - 8, batch - 1300, loss - 130.69418334960938\n",
            "e - 8, batch - 1400, loss - 140.72694396972656\n",
            "e - 8, batch - 1500, loss - 137.63336181640625\n",
            "Epoch: 8, val nll=136.13914743652344\n",
            "e - 8, val loss - 136.13914743652344\n",
            "saved!\n",
            "e - 9\n",
            "e - 9, batch - 0, loss - 127.70584106445312\n",
            "e - 9, batch - 100, loss - 139.676513671875\n",
            "e - 9, batch - 200, loss - 156.81886291503906\n",
            "e - 9, batch - 300, loss - 129.73834228515625\n",
            "e - 9, batch - 400, loss - 135.65716552734375\n",
            "e - 9, batch - 500, loss - 140.41900634765625\n",
            "e - 9, batch - 600, loss - 126.19591522216797\n",
            "e - 9, batch - 700, loss - 134.3702392578125\n",
            "e - 9, batch - 800, loss - 126.7078628540039\n",
            "e - 9, batch - 900, loss - 140.69186401367188\n",
            "e - 9, batch - 1000, loss - 128.54925537109375\n",
            "e - 9, batch - 1100, loss - 138.01400756835938\n",
            "e - 9, batch - 1200, loss - 134.94793701171875\n",
            "e - 9, batch - 1300, loss - 125.85494995117188\n",
            "e - 9, batch - 1400, loss - 130.125\n",
            "e - 9, batch - 1500, loss - 132.54580688476562\n",
            "Epoch: 9, val nll=135.81518828125\n",
            "e - 9, val loss - 135.81518828125\n",
            "saved!\n",
            "e - 10\n",
            "e - 10, batch - 0, loss - 138.61581420898438\n",
            "e - 10, batch - 100, loss - 125.05789184570312\n",
            "e - 10, batch - 200, loss - 131.79541015625\n",
            "e - 10, batch - 300, loss - 129.82177734375\n",
            "e - 10, batch - 400, loss - 124.38038635253906\n",
            "e - 10, batch - 500, loss - 133.5704345703125\n",
            "e - 10, batch - 600, loss - 138.56454467773438\n",
            "e - 10, batch - 700, loss - 146.58160400390625\n",
            "e - 10, batch - 800, loss - 133.26124572753906\n",
            "e - 10, batch - 900, loss - 124.56985473632812\n",
            "e - 10, batch - 1000, loss - 130.88818359375\n",
            "e - 10, batch - 1100, loss - 120.9681396484375\n",
            "e - 10, batch - 1200, loss - 134.0341796875\n",
            "e - 10, batch - 1300, loss - 130.0760498046875\n",
            "e - 10, batch - 1400, loss - 140.70249938964844\n",
            "e - 10, batch - 1500, loss - 129.43519592285156\n",
            "Epoch: 10, val nll=135.62819073486327\n",
            "e - 10, val loss - 135.62819073486327\n",
            "saved!\n",
            "e - 11\n",
            "e - 11, batch - 0, loss - 143.8096923828125\n",
            "e - 11, batch - 100, loss - 132.54534912109375\n",
            "e - 11, batch - 200, loss - 138.42742919921875\n",
            "e - 11, batch - 300, loss - 130.38461303710938\n",
            "e - 11, batch - 400, loss - 140.9510040283203\n",
            "e - 11, batch - 500, loss - 133.84384155273438\n",
            "e - 11, batch - 600, loss - 142.3116912841797\n",
            "e - 11, batch - 700, loss - 135.6187286376953\n",
            "e - 11, batch - 800, loss - 130.0462646484375\n",
            "e - 11, batch - 900, loss - 140.00192260742188\n",
            "e - 11, batch - 1000, loss - 143.4717559814453\n",
            "e - 11, batch - 1100, loss - 127.56885528564453\n",
            "e - 11, batch - 1200, loss - 140.85910034179688\n",
            "e - 11, batch - 1300, loss - 132.83453369140625\n",
            "e - 11, batch - 1400, loss - 139.4776611328125\n",
            "e - 11, batch - 1500, loss - 129.98382568359375\n",
            "Epoch: 11, val nll=135.53460157470704\n",
            "e - 11, val loss - 135.53460157470704\n",
            "saved!\n",
            "e - 12\n",
            "e - 12, batch - 0, loss - 145.99887084960938\n",
            "e - 12, batch - 100, loss - 133.46107482910156\n",
            "e - 12, batch - 200, loss - 125.00303649902344\n",
            "e - 12, batch - 300, loss - 134.17581176757812\n",
            "e - 12, batch - 400, loss - 125.4962158203125\n",
            "e - 12, batch - 500, loss - 132.85264587402344\n",
            "e - 12, batch - 600, loss - 143.50767517089844\n",
            "e - 12, batch - 700, loss - 143.40805053710938\n",
            "e - 12, batch - 800, loss - 136.07643127441406\n",
            "e - 12, batch - 900, loss - 131.40805053710938\n",
            "e - 12, batch - 1000, loss - 135.91213989257812\n",
            "e - 12, batch - 1100, loss - 131.88880920410156\n",
            "e - 12, batch - 1200, loss - 129.94241333007812\n",
            "e - 12, batch - 1300, loss - 135.64544677734375\n",
            "e - 12, batch - 1400, loss - 150.4939422607422\n",
            "e - 12, batch - 1500, loss - 126.26765441894531\n",
            "Epoch: 12, val nll=135.46294982910158\n",
            "e - 12, val loss - 135.46294982910158\n",
            "saved!\n",
            "e - 13\n",
            "e - 13, batch - 0, loss - 127.71903991699219\n",
            "e - 13, batch - 100, loss - 128.42623901367188\n",
            "e - 13, batch - 200, loss - 128.97833251953125\n",
            "e - 13, batch - 300, loss - 124.41452026367188\n",
            "e - 13, batch - 400, loss - 142.5063934326172\n",
            "e - 13, batch - 500, loss - 142.0498046875\n",
            "e - 13, batch - 600, loss - 129.70933532714844\n",
            "e - 13, batch - 700, loss - 136.78097534179688\n",
            "e - 13, batch - 800, loss - 148.72250366210938\n",
            "e - 13, batch - 900, loss - 135.84686279296875\n",
            "e - 13, batch - 1000, loss - 131.8448028564453\n",
            "e - 13, batch - 1100, loss - 123.36839294433594\n",
            "e - 13, batch - 1200, loss - 122.91639709472656\n",
            "e - 13, batch - 1300, loss - 144.47702026367188\n",
            "e - 13, batch - 1400, loss - 151.98617553710938\n",
            "e - 13, batch - 1500, loss - 137.85302734375\n",
            "Epoch: 13, val nll=134.9632642578125\n",
            "e - 13, val loss - 134.9632642578125\n",
            "saved!\n",
            "e - 14\n",
            "e - 14, batch - 0, loss - 141.40536499023438\n",
            "e - 14, batch - 100, loss - 120.88095092773438\n",
            "e - 14, batch - 200, loss - 128.61602783203125\n",
            "e - 14, batch - 300, loss - 135.07638549804688\n",
            "e - 14, batch - 400, loss - 128.8045654296875\n",
            "e - 14, batch - 500, loss - 133.8470458984375\n",
            "e - 14, batch - 600, loss - 135.3726806640625\n",
            "e - 14, batch - 700, loss - 138.94293212890625\n",
            "e - 14, batch - 800, loss - 136.25942993164062\n",
            "e - 14, batch - 900, loss - 122.11317443847656\n",
            "e - 14, batch - 1000, loss - 150.72317504882812\n",
            "e - 14, batch - 1100, loss - 134.82135009765625\n",
            "e - 14, batch - 1200, loss - 125.74150085449219\n",
            "e - 14, batch - 1300, loss - 138.12435913085938\n",
            "e - 14, batch - 1400, loss - 136.08340454101562\n",
            "e - 14, batch - 1500, loss - 133.56332397460938\n",
            "Epoch: 14, val nll=134.96118764648438\n",
            "e - 14, val loss - 134.96118764648438\n",
            "saved!\n",
            "e - 15\n",
            "e - 15, batch - 0, loss - 135.33480834960938\n",
            "e - 15, batch - 100, loss - 132.1816864013672\n",
            "e - 15, batch - 200, loss - 127.05424499511719\n",
            "e - 15, batch - 300, loss - 122.8927001953125\n",
            "e - 15, batch - 400, loss - 131.3397674560547\n",
            "e - 15, batch - 500, loss - 120.74646759033203\n",
            "e - 15, batch - 600, loss - 131.204345703125\n",
            "e - 15, batch - 700, loss - 141.76455688476562\n",
            "e - 15, batch - 800, loss - 128.81240844726562\n",
            "e - 15, batch - 900, loss - 144.56204223632812\n",
            "e - 15, batch - 1000, loss - 129.52139282226562\n",
            "e - 15, batch - 1100, loss - 126.2047348022461\n",
            "e - 15, batch - 1200, loss - 135.8267822265625\n",
            "e - 15, batch - 1300, loss - 139.40292358398438\n",
            "e - 15, batch - 1400, loss - 121.49787902832031\n",
            "e - 15, batch - 1500, loss - 132.02394104003906\n",
            "Epoch: 15, val nll=135.00242546386718\n",
            "e - 15, val loss - 135.00242546386718\n",
            "e - 16\n",
            "e - 16, batch - 0, loss - 136.5094451904297\n",
            "e - 16, batch - 100, loss - 129.7550811767578\n",
            "e - 16, batch - 200, loss - 136.3212127685547\n",
            "e - 16, batch - 300, loss - 132.699462890625\n",
            "e - 16, batch - 400, loss - 136.60345458984375\n",
            "e - 16, batch - 500, loss - 131.07723999023438\n",
            "e - 16, batch - 600, loss - 132.6502685546875\n",
            "e - 16, batch - 700, loss - 136.416015625\n",
            "e - 16, batch - 800, loss - 141.51698303222656\n",
            "e - 16, batch - 900, loss - 135.21636962890625\n",
            "e - 16, batch - 1000, loss - 134.6853790283203\n",
            "e - 16, batch - 1100, loss - 141.6278533935547\n",
            "e - 16, batch - 1200, loss - 130.78067016601562\n",
            "e - 16, batch - 1300, loss - 138.8299560546875\n",
            "e - 16, batch - 1400, loss - 153.05853271484375\n",
            "e - 16, batch - 1500, loss - 137.10157775878906\n",
            "Epoch: 16, val nll=134.89854482421876\n",
            "e - 16, val loss - 134.89854482421876\n",
            "saved!\n",
            "e - 17\n",
            "e - 17, batch - 0, loss - 136.6298065185547\n",
            "e - 17, batch - 100, loss - 135.79598999023438\n",
            "e - 17, batch - 200, loss - 138.83499145507812\n",
            "e - 17, batch - 300, loss - 139.40451049804688\n",
            "e - 17, batch - 400, loss - 128.07583618164062\n",
            "e - 17, batch - 500, loss - 134.67007446289062\n",
            "e - 17, batch - 600, loss - 123.63307189941406\n",
            "e - 17, batch - 700, loss - 115.47262573242188\n",
            "e - 17, batch - 800, loss - 121.30773162841797\n",
            "e - 17, batch - 900, loss - 139.97744750976562\n",
            "e - 17, batch - 1000, loss - 150.00369262695312\n",
            "e - 17, batch - 1100, loss - 120.26383972167969\n",
            "e - 17, batch - 1200, loss - 133.6114959716797\n",
            "e - 17, batch - 1300, loss - 134.25543212890625\n",
            "e - 17, batch - 1400, loss - 134.21142578125\n",
            "e - 17, batch - 1500, loss - 131.0926971435547\n",
            "Epoch: 17, val nll=134.45321340332032\n",
            "e - 17, val loss - 134.45321340332032\n",
            "saved!\n",
            "e - 18\n",
            "e - 18, batch - 0, loss - 119.80582427978516\n",
            "e - 18, batch - 100, loss - 121.57608032226562\n",
            "e - 18, batch - 200, loss - 128.3086395263672\n",
            "e - 18, batch - 300, loss - 122.458740234375\n",
            "e - 18, batch - 400, loss - 146.24581909179688\n",
            "e - 18, batch - 500, loss - 147.6136016845703\n",
            "e - 18, batch - 600, loss - 135.41610717773438\n",
            "e - 18, batch - 700, loss - 134.07098388671875\n",
            "e - 18, batch - 800, loss - 138.23513793945312\n",
            "e - 18, batch - 900, loss - 128.49520874023438\n",
            "e - 18, batch - 1000, loss - 129.20169067382812\n",
            "e - 18, batch - 1100, loss - 134.7747039794922\n",
            "e - 18, batch - 1200, loss - 139.46249389648438\n",
            "e - 18, batch - 1300, loss - 137.24383544921875\n",
            "e - 18, batch - 1400, loss - 127.05887603759766\n",
            "e - 18, batch - 1500, loss - 136.68191528320312\n",
            "Epoch: 18, val nll=134.68686147460937\n",
            "e - 18, val loss - 134.68686147460937\n",
            "e - 19\n",
            "e - 19, batch - 0, loss - 130.9735565185547\n",
            "e - 19, batch - 100, loss - 147.44113159179688\n",
            "e - 19, batch - 200, loss - 130.9061737060547\n",
            "e - 19, batch - 300, loss - 121.25654602050781\n",
            "e - 19, batch - 400, loss - 134.00546264648438\n",
            "e - 19, batch - 500, loss - 122.11441040039062\n",
            "e - 19, batch - 600, loss - 128.79461669921875\n",
            "e - 19, batch - 700, loss - 130.21237182617188\n",
            "e - 19, batch - 800, loss - 142.71356201171875\n",
            "e - 19, batch - 900, loss - 145.02685546875\n",
            "e - 19, batch - 1000, loss - 143.49636840820312\n",
            "e - 19, batch - 1100, loss - 135.544189453125\n",
            "e - 19, batch - 1200, loss - 123.73356628417969\n",
            "e - 19, batch - 1300, loss - 124.7111587524414\n",
            "e - 19, batch - 1400, loss - 139.97528076171875\n",
            "e - 19, batch - 1500, loss - 140.45057678222656\n",
            "Epoch: 19, val nll=134.4170954711914\n",
            "e - 19, val loss - 134.4170954711914\n",
            "saved!\n",
            "e - 20\n",
            "e - 20, batch - 0, loss - 141.40174865722656\n",
            "e - 20, batch - 100, loss - 133.23944091796875\n",
            "e - 20, batch - 200, loss - 134.75022888183594\n",
            "e - 20, batch - 300, loss - 134.564208984375\n",
            "e - 20, batch - 400, loss - 131.16458129882812\n",
            "e - 20, batch - 500, loss - 144.7166748046875\n",
            "e - 20, batch - 600, loss - 128.14361572265625\n",
            "e - 20, batch - 700, loss - 146.94757080078125\n",
            "e - 20, batch - 800, loss - 130.93771362304688\n",
            "e - 20, batch - 900, loss - 168.12042236328125\n",
            "e - 20, batch - 1000, loss - 137.42242431640625\n",
            "e - 20, batch - 1100, loss - 142.27731323242188\n",
            "e - 20, batch - 1200, loss - 131.58502197265625\n",
            "e - 20, batch - 1300, loss - 136.8673553466797\n",
            "e - 20, batch - 1400, loss - 124.72309875488281\n",
            "e - 20, batch - 1500, loss - 126.9715347290039\n",
            "Epoch: 20, val nll=134.4407585205078\n",
            "e - 20, val loss - 134.4407585205078\n",
            "e - 21\n",
            "e - 21, batch - 0, loss - 130.96939086914062\n",
            "e - 21, batch - 100, loss - 136.89666748046875\n",
            "e - 21, batch - 200, loss - 128.93763732910156\n",
            "e - 21, batch - 300, loss - 143.9972381591797\n",
            "e - 21, batch - 400, loss - 134.55072021484375\n",
            "e - 21, batch - 500, loss - 114.35174560546875\n",
            "e - 21, batch - 600, loss - 142.92977905273438\n",
            "e - 21, batch - 700, loss - 130.96884155273438\n",
            "e - 21, batch - 800, loss - 146.63250732421875\n",
            "e - 21, batch - 900, loss - 149.3912353515625\n",
            "e - 21, batch - 1000, loss - 144.89398193359375\n",
            "e - 21, batch - 1100, loss - 140.760986328125\n",
            "e - 21, batch - 1200, loss - 129.75970458984375\n",
            "e - 21, batch - 1300, loss - 144.94253540039062\n",
            "e - 21, batch - 1400, loss - 138.20257568359375\n",
            "e - 21, batch - 1500, loss - 150.10342407226562\n",
            "Epoch: 21, val nll=134.51753055419923\n",
            "e - 21, val loss - 134.51753055419923\n",
            "e - 22\n",
            "e - 22, batch - 0, loss - 159.54766845703125\n",
            "e - 22, batch - 100, loss - 132.8137664794922\n",
            "e - 22, batch - 200, loss - 128.30429077148438\n",
            "e - 22, batch - 300, loss - 136.11302185058594\n",
            "e - 22, batch - 400, loss - 133.05563354492188\n",
            "e - 22, batch - 500, loss - 134.05758666992188\n",
            "e - 22, batch - 600, loss - 139.1646270751953\n",
            "e - 22, batch - 700, loss - 127.10214233398438\n",
            "e - 22, batch - 800, loss - 138.03369140625\n",
            "e - 22, batch - 900, loss - 128.084228515625\n",
            "e - 22, batch - 1000, loss - 117.67254638671875\n",
            "e - 22, batch - 1100, loss - 135.27459716796875\n",
            "e - 22, batch - 1200, loss - 131.32888793945312\n",
            "e - 22, batch - 1300, loss - 143.6730194091797\n",
            "e - 22, batch - 1400, loss - 122.79701232910156\n",
            "e - 22, batch - 1500, loss - 137.10879516601562\n",
            "Epoch: 22, val nll=134.42867819824218\n",
            "e - 22, val loss - 134.42867819824218\n",
            "e - 23\n",
            "e - 23, batch - 0, loss - 127.74555969238281\n",
            "e - 23, batch - 100, loss - 126.84166717529297\n",
            "e - 23, batch - 200, loss - 114.42547607421875\n",
            "e - 23, batch - 300, loss - 129.56088256835938\n",
            "e - 23, batch - 400, loss - 135.35018920898438\n",
            "e - 23, batch - 500, loss - 128.55125427246094\n",
            "e - 23, batch - 600, loss - 132.70750427246094\n",
            "e - 23, batch - 700, loss - 131.06016540527344\n",
            "e - 23, batch - 800, loss - 128.072265625\n",
            "e - 23, batch - 900, loss - 130.7816162109375\n",
            "e - 23, batch - 1000, loss - 138.88528442382812\n",
            "e - 23, batch - 1100, loss - 132.36289978027344\n",
            "e - 23, batch - 1200, loss - 141.29229736328125\n",
            "e - 23, batch - 1300, loss - 133.57852172851562\n",
            "e - 23, batch - 1400, loss - 131.662841796875\n",
            "e - 23, batch - 1500, loss - 140.18441772460938\n",
            "Epoch: 23, val nll=134.24505502929688\n",
            "e - 23, val loss - 134.24505502929688\n",
            "saved!\n",
            "e - 24\n",
            "e - 24, batch - 0, loss - 133.17523193359375\n",
            "e - 24, batch - 100, loss - 125.4753646850586\n",
            "e - 24, batch - 200, loss - 140.873046875\n",
            "e - 24, batch - 300, loss - 135.15252685546875\n",
            "e - 24, batch - 400, loss - 125.02042388916016\n",
            "e - 24, batch - 500, loss - 139.69082641601562\n",
            "e - 24, batch - 600, loss - 147.63385009765625\n",
            "e - 24, batch - 700, loss - 131.8452911376953\n",
            "e - 24, batch - 800, loss - 136.6995086669922\n",
            "e - 24, batch - 900, loss - 145.187744140625\n",
            "e - 24, batch - 1000, loss - 140.21319580078125\n",
            "e - 24, batch - 1100, loss - 133.7281951904297\n",
            "e - 24, batch - 1200, loss - 133.5926513671875\n",
            "e - 24, batch - 1300, loss - 127.76991271972656\n",
            "e - 24, batch - 1400, loss - 142.02267456054688\n",
            "e - 24, batch - 1500, loss - 137.16629028320312\n",
            "Epoch: 24, val nll=134.21114982910157\n",
            "e - 24, val loss - 134.21114982910157\n",
            "saved!\n",
            "e - 25\n",
            "e - 25, batch - 0, loss - 133.08790588378906\n",
            "e - 25, batch - 100, loss - 132.7866668701172\n",
            "e - 25, batch - 200, loss - 140.672607421875\n",
            "e - 25, batch - 300, loss - 145.13845825195312\n",
            "e - 25, batch - 400, loss - 145.4118194580078\n",
            "e - 25, batch - 500, loss - 128.22805786132812\n",
            "e - 25, batch - 600, loss - 133.059814453125\n",
            "e - 25, batch - 700, loss - 145.6619873046875\n",
            "e - 25, batch - 800, loss - 120.19123840332031\n",
            "e - 25, batch - 900, loss - 142.20513916015625\n",
            "e - 25, batch - 1000, loss - 136.00296020507812\n",
            "e - 25, batch - 1100, loss - 142.4665985107422\n",
            "e - 25, batch - 1200, loss - 129.37318420410156\n",
            "e - 25, batch - 1300, loss - 135.16751098632812\n",
            "e - 25, batch - 1400, loss - 138.9866943359375\n",
            "e - 25, batch - 1500, loss - 149.58218383789062\n",
            "Epoch: 25, val nll=134.29842419433595\n",
            "e - 25, val loss - 134.29842419433595\n",
            "e - 26\n",
            "e - 26, batch - 0, loss - 130.21197509765625\n",
            "e - 26, batch - 100, loss - 131.46502685546875\n",
            "e - 26, batch - 200, loss - 145.46487426757812\n",
            "e - 26, batch - 300, loss - 136.45477294921875\n",
            "e - 26, batch - 400, loss - 128.36599731445312\n",
            "e - 26, batch - 500, loss - 130.49119567871094\n",
            "e - 26, batch - 600, loss - 137.75698852539062\n",
            "e - 26, batch - 700, loss - 134.0919189453125\n",
            "e - 26, batch - 800, loss - 125.7508773803711\n",
            "e - 26, batch - 900, loss - 122.33973693847656\n",
            "e - 26, batch - 1000, loss - 129.00714111328125\n",
            "e - 26, batch - 1100, loss - 137.67626953125\n",
            "e - 26, batch - 1200, loss - 137.22621154785156\n",
            "e - 26, batch - 1300, loss - 142.78167724609375\n",
            "e - 26, batch - 1400, loss - 146.12196350097656\n",
            "e - 26, batch - 1500, loss - 126.58268737792969\n",
            "Epoch: 26, val nll=134.00787213134765\n",
            "e - 26, val loss - 134.00787213134765\n",
            "saved!\n",
            "e - 27\n",
            "e - 27, batch - 0, loss - 130.63453674316406\n",
            "e - 27, batch - 100, loss - 130.43490600585938\n",
            "e - 27, batch - 200, loss - 132.8016815185547\n",
            "e - 27, batch - 300, loss - 137.3717803955078\n",
            "e - 27, batch - 400, loss - 121.22930145263672\n",
            "e - 27, batch - 500, loss - 135.70791625976562\n",
            "e - 27, batch - 600, loss - 115.13037109375\n",
            "e - 27, batch - 700, loss - 130.66876220703125\n",
            "e - 27, batch - 800, loss - 127.1202163696289\n",
            "e - 27, batch - 900, loss - 142.82888793945312\n",
            "e - 27, batch - 1000, loss - 148.35800170898438\n",
            "e - 27, batch - 1100, loss - 124.95295715332031\n",
            "e - 27, batch - 1200, loss - 145.28878784179688\n",
            "e - 27, batch - 1300, loss - 131.16326904296875\n",
            "e - 27, batch - 1400, loss - 138.64309692382812\n",
            "e - 27, batch - 1500, loss - 126.7350845336914\n",
            "Epoch: 27, val nll=134.1200626953125\n",
            "e - 27, val loss - 134.1200626953125\n",
            "e - 28\n",
            "e - 28, batch - 0, loss - 131.56781005859375\n",
            "e - 28, batch - 100, loss - 134.06634521484375\n",
            "e - 28, batch - 200, loss - 122.6722640991211\n",
            "e - 28, batch - 300, loss - 132.121826171875\n",
            "e - 28, batch - 400, loss - 141.8820037841797\n",
            "e - 28, batch - 500, loss - 138.62451171875\n",
            "e - 28, batch - 600, loss - 140.53289794921875\n",
            "e - 28, batch - 700, loss - 138.62615966796875\n",
            "e - 28, batch - 800, loss - 123.27427673339844\n",
            "e - 28, batch - 900, loss - 114.86893463134766\n",
            "e - 28, batch - 1000, loss - 131.1351318359375\n",
            "e - 28, batch - 1100, loss - 129.84080505371094\n",
            "e - 28, batch - 1200, loss - 132.37890625\n",
            "e - 28, batch - 1300, loss - 135.36976623535156\n",
            "e - 28, batch - 1400, loss - 139.1533203125\n",
            "e - 28, batch - 1500, loss - 137.60244750976562\n",
            "Epoch: 28, val nll=133.8798212036133\n",
            "e - 28, val loss - 133.8798212036133\n",
            "saved!\n",
            "e - 29\n",
            "e - 29, batch - 0, loss - 135.18955993652344\n",
            "e - 29, batch - 100, loss - 141.93972778320312\n",
            "e - 29, batch - 200, loss - 142.28692626953125\n",
            "e - 29, batch - 300, loss - 122.80369567871094\n",
            "e - 29, batch - 400, loss - 121.23796844482422\n",
            "e - 29, batch - 500, loss - 140.2865753173828\n",
            "e - 29, batch - 600, loss - 130.8165283203125\n",
            "e - 29, batch - 700, loss - 124.71385192871094\n",
            "e - 29, batch - 800, loss - 130.62208557128906\n",
            "e - 29, batch - 900, loss - 133.16900634765625\n",
            "e - 29, batch - 1000, loss - 112.44154357910156\n",
            "e - 29, batch - 1100, loss - 112.08882904052734\n",
            "e - 29, batch - 1200, loss - 134.76022338867188\n",
            "e - 29, batch - 1300, loss - 132.66632080078125\n",
            "e - 29, batch - 1400, loss - 126.58736419677734\n",
            "e - 29, batch - 1500, loss - 138.02035522460938\n",
            "Epoch: 29, val nll=133.73919610595703\n",
            "e - 29, val loss - 133.73919610595703\n",
            "saved!\n",
            "e - 30\n",
            "e - 30, batch - 0, loss - 127.76446533203125\n",
            "e - 30, batch - 100, loss - 130.5745849609375\n",
            "e - 30, batch - 200, loss - 119.98057556152344\n",
            "e - 30, batch - 300, loss - 142.9967041015625\n",
            "e - 30, batch - 400, loss - 126.6749038696289\n",
            "e - 30, batch - 500, loss - 144.08074951171875\n",
            "e - 30, batch - 600, loss - 127.94923400878906\n",
            "e - 30, batch - 700, loss - 134.31765747070312\n",
            "e - 30, batch - 800, loss - 138.01873779296875\n",
            "e - 30, batch - 900, loss - 149.0548858642578\n",
            "e - 30, batch - 1000, loss - 138.35067749023438\n",
            "e - 30, batch - 1100, loss - 128.20697021484375\n",
            "e - 30, batch - 1200, loss - 148.9152374267578\n",
            "e - 30, batch - 1300, loss - 125.1500015258789\n",
            "e - 30, batch - 1400, loss - 126.86891174316406\n",
            "e - 30, batch - 1500, loss - 135.34796142578125\n",
            "Epoch: 30, val nll=133.92177957763673\n",
            "e - 30, val loss - 133.92177957763673\n",
            "e - 31\n",
            "e - 31, batch - 0, loss - 127.58773803710938\n",
            "e - 31, batch - 100, loss - 127.74307250976562\n",
            "e - 31, batch - 200, loss - 139.18008422851562\n",
            "e - 31, batch - 300, loss - 143.35140991210938\n",
            "e - 31, batch - 400, loss - 132.2349853515625\n",
            "e - 31, batch - 500, loss - 128.54165649414062\n",
            "e - 31, batch - 600, loss - 122.00790405273438\n",
            "e - 31, batch - 700, loss - 146.61143493652344\n",
            "e - 31, batch - 800, loss - 138.0429229736328\n",
            "e - 31, batch - 900, loss - 142.79653930664062\n",
            "e - 31, batch - 1000, loss - 122.54112243652344\n",
            "e - 31, batch - 1100, loss - 131.02610778808594\n",
            "e - 31, batch - 1200, loss - 130.15072631835938\n",
            "e - 31, batch - 1300, loss - 135.7135009765625\n",
            "e - 31, batch - 1400, loss - 130.08938598632812\n",
            "e - 31, batch - 1500, loss - 140.54742431640625\n",
            "Epoch: 31, val nll=133.66330732421875\n",
            "e - 31, val loss - 133.66330732421875\n",
            "saved!\n",
            "e - 32\n",
            "e - 32, batch - 0, loss - 126.48611450195312\n",
            "e - 32, batch - 100, loss - 138.705810546875\n",
            "e - 32, batch - 200, loss - 132.81375122070312\n",
            "e - 32, batch - 300, loss - 149.933837890625\n",
            "e - 32, batch - 400, loss - 134.28616333007812\n",
            "e - 32, batch - 500, loss - 133.40452575683594\n",
            "e - 32, batch - 600, loss - 143.9920654296875\n",
            "e - 32, batch - 700, loss - 135.72642517089844\n",
            "e - 32, batch - 800, loss - 133.03343200683594\n",
            "e - 32, batch - 900, loss - 137.2351531982422\n",
            "e - 32, batch - 1000, loss - 120.99412536621094\n",
            "e - 32, batch - 1100, loss - 130.21328735351562\n",
            "e - 32, batch - 1200, loss - 133.60011291503906\n",
            "e - 32, batch - 1300, loss - 131.06724548339844\n",
            "e - 32, batch - 1400, loss - 129.6085662841797\n",
            "e - 32, batch - 1500, loss - 112.08892822265625\n",
            "Epoch: 32, val nll=134.08638416748047\n",
            "e - 32, val loss - 134.08638416748047\n",
            "e - 33\n",
            "e - 33, batch - 0, loss - 125.64360046386719\n",
            "e - 33, batch - 100, loss - 139.802734375\n",
            "e - 33, batch - 200, loss - 138.429931640625\n",
            "e - 33, batch - 300, loss - 139.80258178710938\n",
            "e - 33, batch - 400, loss - 136.45533752441406\n",
            "e - 33, batch - 500, loss - 116.77217864990234\n",
            "e - 33, batch - 600, loss - 122.25361633300781\n",
            "e - 33, batch - 700, loss - 138.49917602539062\n",
            "e - 33, batch - 800, loss - 134.3382568359375\n",
            "e - 33, batch - 900, loss - 131.64511108398438\n",
            "e - 33, batch - 1000, loss - 138.96063232421875\n",
            "e - 33, batch - 1100, loss - 124.31045532226562\n",
            "e - 33, batch - 1200, loss - 140.7064666748047\n",
            "e - 33, batch - 1300, loss - 135.40005493164062\n",
            "e - 33, batch - 1400, loss - 136.12074279785156\n",
            "e - 33, batch - 1500, loss - 131.34913635253906\n",
            "Epoch: 33, val nll=133.63752548828126\n",
            "e - 33, val loss - 133.63752548828126\n",
            "saved!\n",
            "e - 34\n",
            "e - 34, batch - 0, loss - 134.0684356689453\n",
            "e - 34, batch - 100, loss - 145.60983276367188\n",
            "e - 34, batch - 200, loss - 124.76876068115234\n",
            "e - 34, batch - 300, loss - 128.53965759277344\n",
            "e - 34, batch - 400, loss - 131.36692810058594\n",
            "e - 34, batch - 500, loss - 122.58797454833984\n",
            "e - 34, batch - 600, loss - 131.52658081054688\n",
            "e - 34, batch - 700, loss - 131.87997436523438\n",
            "e - 34, batch - 800, loss - 121.16754150390625\n",
            "e - 34, batch - 900, loss - 138.351806640625\n",
            "e - 34, batch - 1000, loss - 135.40602111816406\n",
            "e - 34, batch - 1100, loss - 133.73269653320312\n",
            "e - 34, batch - 1200, loss - 139.5361328125\n",
            "e - 34, batch - 1300, loss - 131.48043823242188\n",
            "e - 34, batch - 1400, loss - 138.83157348632812\n",
            "e - 34, batch - 1500, loss - 140.15777587890625\n"
          ]
        }
      ],
      "source": [
        "# DO NOT REMOVE OR MODIFY\n",
        "# Training procedure\n",
        "nll_val = training(name=result_dir + name, max_patience=max_patience,\n",
        "                   num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
        "                   training_loader=train_loader, val_loader=val_loader,\n",
        "                   shape=(28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAuMt9_wquOI"
      },
      "outputs": [],
      "source": [
        "# DO NOT REMOVE OR MODIFY\n",
        "# Final evaluation\n",
        "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
        "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
        "f.write(str(test_loss))\n",
        "f.close()\n",
        "\n",
        "samples_real(result_dir + name, test_loader)\n",
        "samples_generated(result_dir + name, test_loader, extra_name='_FINAL')\n",
        "\n",
        "plot_curve(result_dir + name, nll_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaBwGtSJF8ag"
      },
      "source": [
        "### Results and discussion\n",
        "\n",
        "After a successful training of your model, we would like to ask you to present your data and analyze it. Please answer the following questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4WZkoiHFyZm"
      },
      "source": [
        "\n",
        "#### Question 8 (1 pt)\n",
        "\n",
        "Please select the real data, and the final generated data and include them in this report. Please comment on the following:\n",
        "- (0.5 pt) Do you think the model was trained properly by looking at the generations? Please motivate your answer well.\n",
        "- (0.5 pt) What are potential problems with evaluating a generative model by looking at generated data? How can we evalute generative models (ELBO or NLL do not count as an answer)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8yQ2T9GIuvc"
      },
      "source": [
        "ANSWER: [Please fill in]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmyH318fIwc9"
      },
      "source": [
        "#### Question 9 (1.25 pt)\n",
        "\n",
        "Please include the plot of the negative ELBO. Please comment on the following:\n",
        "- (0.25 pt) Is the training of your VAE stable or unstable? Why?\n",
        "- (1 pt) What is the influence of the optimizer on your model? Do the hyperparameter values of the optimizer important and how do they influence the training? Motivate well your answer (e.g., run the script with more than one learning rate and present two plots here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-10GAVZtKTj2"
      },
      "source": [
        "ANSWER: [Please fill in]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7QhyAMms8-L"
      },
      "source": [
        "# Grading (10pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juIdxXhos-mV"
      },
      "source": [
        "- Question 0: 3pt\n",
        "- Question 1: 0.5pt\n",
        "- Question 2: 0.25pt\n",
        "- Question 3: 0.5pt\n",
        "- Question 4: 0.5pt\n",
        "- Question 5: 2pt\n",
        "- Question 6: 0.5pt\n",
        "- Question 7: 0.5pt\n",
        "- Question 8: 1pt\n",
        "- Question 9: 1.25pt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}